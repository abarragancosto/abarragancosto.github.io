---
title: "De documentaci√≥n y capturas al .feature: IA local con Llama 3.2 + Llava"
excerpt: "Genera escenarios Gherkin (o casos de uso) sin filtrar tus datos a la nube."
classes: wide
toc: true
date: 2025-05-19
header:
  overlay_filter: 0.25
  caption: "QA √ó IA Weekly ¬∑ Art√≠culo 2"
---
 
> En este art√≠culo te muestro c√≥mo montar un flujo completo de generaci√≥n de escenarios .feature (Gherkin) y casos de uso a partir de requisitos funcionales y capturas de pantalla, todo ejecutado 100% en local, sin necesidad de enviar datos sensibles a servicios externos. Esto elimina las barreras legales y t√©cnicas que muchas empresas ponen cuando se quiere usar IA con datos internos.  
> Gracias a herramientas como Ollama, puedes tener modelos LLM como Llama 3.2 (para texto) y Llava (para visi√≥n) corriendo directamente en tu equipo, sin pagar APIs, sin exponer datos, sin depender de internet...  
> Este primer post no solo sirve como ejemplo pr√°ctico para profesionales de QA, sino como gu√≠a de c√≥mo desplegar e integrar modelos locales de forma sencilla. Es el primer paso de una serie en la que exploraremos c√≥mo pasar de requisitos a c√≥digo de automatizaci√≥n real generado por IA.  
> Si trabajas en QA, desarrollo o est√°s explorando c√≥mo usar IA sin comprometer la privacidad de tus proyectos, este art√≠culo es para ti.  

> Modelos que se utilizar√°n:
> 
> | Tarea | Modelo        | Peso Q4 | RAM real |
> |-------|---------------|--------|----------|
> | Texto ‚Üí Gherkin | **Llama 3.2** | 4.4 GB | 6‚Äì8 GB   |
> | Imagen + texto ‚Üí Gherkin | **Llava**     | 4.7 GB | 7-8 GB   |


## 1. ¬øPor qu√© ejecutar la IA **en local**?

1. **Privacidad y cumplimiento**¬†¬†Los requisitos y los mock-ups nunca salen de tu equipo, as√≠ que no hay riesgo de que queden guardados en los registros de un servicio externo. Puedes incluso ejecutarlos sin conexi√≥n a internet.  
2. **Coste cero¬†& offline**¬†¬†Sin cuotas ni l√≠mites de API. No pagas tokens por petici√≥n, no temes sobrecostes y puedes realizar tantas iteraciones como quieras mientras ajustas el prompt.  
3. **Iteraci√≥n r√°pida**¬†¬†Modificas prompts, re‚Äëcorres, comparas‚Ä¶ sin esperar cola externa.
4. **Independencia**¬†¬†Si ma√±ana cambian precios o pol√≠ticas de una API, tu flujo sigue igual. Adem√°s, puedes afinar el modelo con ejemplos propios sin enviarlos a nadie.

## 2. Setup
En esta secci√≥n vamos a preparar el entorno necesario para ejecutar nuestro generador de escenarios `.feature` usando modelos LLM locales.
### ¬øQu√© es Ollama?
**[Ollama](https://ollama.com/)** es una herramienta que permite ejecutar modelos de lenguaje (LLM) de forma **local** en tu equipo, sin necesidad de depender de servicios en la nube. Ofrece multitud de m√≥dulos de diferentes tama√±os y prop√≥sitos.

### 2.1. Instalar Ollama
Dependiendo de tu sistema operativo, puedes instalar Ollama de dos formas:
#### Opci√≥n 1. Instalaci√≥n r√°pida por terminal
```bash
# Para macOS
brew install ollama

# Para Windows
winget install Ollama.Ollama

# Para Linux
curl -fsSL https://ollama.com/install.sh | sh
```

> **Nota**: Necesitar√°s tener instalado `brew` (macOS), `winget` (Windows) o `curl` (Linux). Si no los tienes, consulta su documentaci√≥n correspondiente.

#### Opci√≥n 2. Instalaci√≥n manual desde su web
Tambi√©n puedes instalar Ollama como cualquier otro programa, descarg√°ndolo directamente desde su web oficial:
**[Descargar Ollama](https://ollama.com/download)**

Solo tienes que ir a la secci√≥n de instalaci√≥n, descargar el instalador correspondiente a tu sistema operativo y seguir los pasos habituales.

---

### 2.2. Iniciar el servidor de Ollama

Una vez instalado Ollama, es necesario arrancar el servidor local, que es el encargado de gestionar los modelos y responder a nuestras peticiones.
#### **Si lo instalaste desde la terminal**
Ejecuta el siguiente comando para lanzar el servidor en segundo plano:
```bash
ollama serve &
```
Esto dejar√° el servidor corriendo y listo para recibir peticiones desde otros scripts o herramientas.
#### **Si lo instalaste manualmente desde la web**
Simplemente abre la aplicaci√≥n de Ollama como cualquier otro programa en tu sistema. Esto pondr√° en marcha el servidor de forma autom√°tica.

> **Importante**: A partir de este punto, todos los pasos siguientes funcionar√°n exactamente igual, independientemente del m√©todo de instalaci√≥n que hayas elegido.


### 2.3. Descargar modelos LLM

Una vez iniciado el servidor, vamos a descargar los modelos que utilizaremos para generar nuestros escenarios `.feature` y para describir interfaces a partir de im√°genes:

```bash
ollama pull llama3.2
ollama pull llava
```

#### ¬øPor qu√© estos modelos?
- **`llama3.2`**: modelo de texto optimizado para tareas complejas como generaci√≥n de c√≥digo, razonamiento y QA avanzado. Lo utilizaremos para generar escenarios en lenguaje Gherkin. Aunque es un modelo grande para el uso puntual que haremos aqu√≠, lo descargamos desde el principio para tenerlo disponible en local y reutilizarlo m√°s adelante, por ejemplo, para generar c√≥digo de automatizaci√≥n con Selenium en futuros proyectos.
- **`llava`**: modelo multimodal que puede interpretar tanto texto como im√°genes. Lo utilizamos espec√≠ficamente para extraer descripciones detalladas de interfaces a partir de capturas de pantalla. Su capacidad de visi√≥n lo hace ideal para este tipo de tareas sin depender de soluciones en la nube.



---

### 2.4. Clonar el repositorio del proyecto

Clonamos el proyecto base que contiene toda la l√≥gica para interactuar con los modelos, pasarles los prompts adecuados y generar archivos `.feature` autom√°ticamente.

```bash
git clone https://github.com/abarragancosto/llm-feature-gen.git
cd llm-feature-gen
```

> En este repositorio encontrar√°s:
> - Scripts para lanzar peticiones a los modelos
> - Templates de prompts optimizados
> - Utilidades para guardar las salidas directamente como archivos `.feature`

---

### 2.5. Crear entorno virtual e instalar dependencias

Creamos un entorno virtual para aislar las dependencias del proyecto:

```bash
python -m venv .venv && source .venv/bin/activate
```

Y luego instalamos todos los paquetes necesarios:

```bash
pip install -r requirements.txt
```

> Esto incluye bibliotecas para interactuar con Ollama, procesar im√°genes, manejar prompts, etc.

## 3. Ejecutar programa

Una vez descargados los modelos y arrancado el servidor de Ollama, ya puedes utilizar el script `generate_features.py` para generar autom√°ticamente escenarios en formato Gherkin (`.feature`) o descripciones de casos de uso a partir de texto y, opcionalmente, im√°genes de mock-up.

---

### Argumentos disponibles

El script admite varios par√°metros para personalizar la ejecuci√≥n:

| Par√°metro           | Descripci√≥n                                                                                         |
|---------------------|-----------------------------------------------------------------------------------------------------|
| `requirements`      | Ruta al archivo de requisitos (en formato `.txt` o `.md`). **Obligatorio**.                         |
| `-m`, `--mockup`     | Ruta a una imagen de la interfaz si se desea usar un modelo con capacidad visual. (Opcional) |
| `-n`, `--num`        | N√∫mero de escenarios o casos a generar (por defecto: 5). (Opcional)                                 |
| `-o`, `--output`     | Nombre del archivo de salida. Por defecto: `output.feature`.                                        |
| `-f`, `--format`     | Formato de salida: `feature` para Gherkin o `txt` para casos de uso en texto plano. **Obligatorio** |
| `--prompt-file`      | Ruta a un archivo con prompt personalizado (opcional).                                              |

---

### 3.1 Solo texto

Si solo tienes una descripci√≥n funcional (sin mock-up) y quieres generar escenarios en formato `.feature`, ejecuta:

```bash
python generate_features.py examples/requirements.txt -o login.feature
```

Esto usar√° el archivo `examples/requirements.txt`, generar√° 5 escenarios (valor por defecto) y los guardar√° en `login.feature`.

---

Como demostraci√≥n del ejemplo, el fichero `requirements.txt` contiene lo siguiente:

![Diagrama flujo](/assets/images/articulo2/requirements.png)

Una vez ejecutado, por consola nos saldr√°:

![Ejecuci√≥n solo texto](/assets/images/articulo2/ejecuci√≥n1.png)

El resultado de la feature ser√°:

![Resultado solo texto](/assets/images/articulo2/resultado1.png)

---

### 3.2 Texto **+** mock‚Äëup

Si adem√°s del archivo de requisitos tienes una imagen de la interfaz (mock-up), puedes a√±adirla para que el modelo la interprete y genere escenarios m√°s ajustados visualmente:

```bash
python generate_features.py examples/requirements.txt --mockup examples/image.png -n 8 -o loginMockup.feature
```

üìå Esto generar√° 8 escenarios combinando texto e imagen, y los guardar√° en `loginMockup.feature`.

La consola mostrar√° el resultado de la interpretaci√≥n de imagen a texto:

![Imagen a texto](/assets/images/articulo2/imagen-texto.png)

El resultado de la feature ser√≠a:

![Resultado con imagen](/assets/images/articulo2/resultado2.png)

---

### Ejemplo con salida en texto plano

Si no quieres escenarios en Gherkin sino casos de uso en un txt, cambia el formato de salida con `--format txt`:

```bash
python generate_features.py examples/requirements.txt -f txt -o casos_uso.txt
```

---

### Usar un prompt personalizado

Puedes utilizar tu propio archivo de prompt (por ejemplo, para adaptar el estilo o el idioma):

```bash
python generate_features.py examples/requirements.txt --prompt-file custom_prompt.txt
```

## 4. Comentarios finales

- **La revisi√≥n humana es clave**: Aunque el uso de modelos LLM agiliza notablemente la generaci√≥n de escenarios y casos de uso, **no sustituye el criterio de un buen QA**. Lo generado puede servir como punto de partida, inspiraci√≥n o repaso de casos que podr√≠an haberse pasado por alto, pero siempre debe haber una validaci√≥n manual.

- **Versi√≥n ligera recomendada**: Si `llava` te resulta pesado en tu equipo, te recomiendo probar la versi√≥n optimizada `llava:7b-v1.5-q4_0`. Es la que he utilizado para las pruebas y ofrece un excelente equilibrio entre rendimiento y calidad.

- **Objetivo de este post**: Esta primera entrega ha sido una toma de contacto para mostrar c√≥mo puedes trabajar con modelos LLM de forma **local**, sin depender de APIs externas, y generar `.feature` files de forma autom√°tica a partir de requisitos.

- **Lo realmente valioso**: M√°s all√° de automatizar la generaci√≥n de archivos `.feature`, lo importante aqu√≠ es mostrar que **tener modelos potentes en tu m√°quina local es posible, gratuito y muy √∫til en flujos QA reales**.

---

## 5. Pr√≥ximos pasos

En pr√≥ximos art√≠culos, daremos un paso m√°s: **convertiremos estos archivos `.feature` en c√≥digo Selenium (Java)** utilizando IA, incluyendo estructuras Page Object y sugerencias de selectores autom√°ticos.

Este flujo completo permitir√° generar desde requisitos hasta pruebas automatizadas listas para ejecuci√≥n, todo asistido por modelos LLM.

---

## 6. ¬øTe animas a dar feedback?

¬øHas probado este flujo? Me encantar√≠a saber tu opini√≥n:

- ¬øTe ha resultado √∫til este post?
- ¬øTe gustar√≠a m√°s profundidad t√©cnica o que fuera m√°s directo al grano?
- ¬øTe interesa que detalle m√°s las herramientas utilizadas?
- ¬øEchas de menos m√°s ejemplos o capturas?

Tu feedback me ayuda a mejorar cada contenido. ¬°Gracias por llegar hasta aqu√≠ y ser parte de este experimento con LLMs en QA! üôå

